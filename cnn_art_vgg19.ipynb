{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn-art_vgg19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmPhvbzPwajL8Bla1gFdM2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyanique/cnn-art/blob/master/cnn_art_vgg19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Mt3WNsfaNnz",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qzWqLmNaG1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Python Print\n",
        "from __future__ import print_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRVhpZONaVXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqE5Ieqiag3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Image Loader and Display\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTrsF5-UbB41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Train/Load/Copy etc\n",
        "import torchvision.models as models\n",
        "import copy\n",
        "import os\n",
        "import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQBwC9AkVvdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ignore ipykernel warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# warnings.filterwarnings(action='once')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSVTOf12bKYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GPU Settings\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS0hAjI1axiS",
        "colab_type": "text"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyUXOBBqbYPr",
        "colab_type": "text"
      },
      "source": [
        "### Helper functions for images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoGEZ_sRbX0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# image sizes: use smaller one if on CPU\n",
        "imsize = 512 if torch.cuda.is_available() else 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjS4gemZa-ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# image loader function\n",
        "def image_loader(img_name):\n",
        "  image = Image.open(img_name)\n",
        "  loader = transforms.Compose([transforms.Resize(imsize),transforms.ToTensor()])\n",
        "  image_tensor = loader(image).unsqueeze(0)\n",
        "  return image_tensor.to(device, torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XbLfb3ykhh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# image display or save\n",
        "def image_show(img_tensor, title=None,save_fig=False):\n",
        "  image = img_tensor.cpu().clone()\n",
        "  image = image.squeeze(0)\n",
        "  image = (transforms.ToPILImage())(image)\n",
        "  img_title = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") if (title is None) else title\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.title(img_title)\n",
        "  if save_fig:\n",
        "    plt.savefig(\"plots/{}.png\".format(img_title))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxawldBGkY_Q",
        "colab_type": "text"
      },
      "source": [
        "### A Pair of Stylier Image and Content Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-Zqu_rVca_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A style image and a content image\n",
        "stylier_img = image_loader(\"img_data/Antoine_Watteau-L'imbarco_per_Citera.jpg\")\n",
        "content_img = image_loader(\"img_data/tea-3190241.jpg\")\n",
        "\n",
        "assert stylier_img.size() == content_img.size(), \"stylier image and content image should of same size\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD5cgQqMkeXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the loaded images\n",
        "plt.ion()\n",
        "plt.figure()\n",
        "image_show(stylier_img,\"Antoine_Watteau - styler\",save_fig=True)\n",
        "plt.figure()\n",
        "image_show(content_img,save_fig=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7CyIOOjmC8J",
        "colab_type": "text"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqKBTFPomF0u",
        "colab_type": "text"
      },
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkzHCg5omdEk",
        "colab_type": "text"
      },
      "source": [
        "MSE for content loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t8_IzA-mHQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContentLoss(nn.Module):\n",
        "  def __init__(self, target,):\n",
        "    super(ContentLoss, self).__init__()\n",
        "    # we 'detach' the target content from the tree used\n",
        "    # to dynamically compute the gradient: this is a stated value,\n",
        "    # not a variable. Otherwise the forward method of the criterion\n",
        "    # will throw an error.\n",
        "    self.target = target.detach()\n",
        "  \n",
        "  def forward(self, input):\n",
        "    self.loss = F.mse_loss(input, self.target)\n",
        "    #self.loss = F.binary_cross_entropy_with_logits(input, self.target)\n",
        "    return input  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA7r6oEwmm5G",
        "colab_type": "text"
      },
      "source": [
        "MSE for styler loss (need to use Gram matrix)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhwdfQdAmvCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram_matrix(input):\n",
        "  a,b,c,d = input.size()  #a=batchsize, b=n_featuremaps, cxd=dimesnion of a feature map\n",
        "  features = input.view(a*b, c*d)\n",
        "  gram = torch.mm(features, features.t())\n",
        "  return gram.div(a*b*c*d)\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "  def __init__(self, target_feature):\n",
        "    super(StyleLoss, self).__init__()\n",
        "    self.target = gram_matrix(target_feature).detach()\n",
        "\n",
        "  def forward(self, input):\n",
        "    G = gram_matrix(input)\n",
        "    self.loss = F.mse_loss(G, self.target)\n",
        "    return input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtfghPdinpM7",
        "colab_type": "text"
      },
      "source": [
        "### Transfer Learning with Pre-Trained VGG19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2KJtI3IoBC9",
        "colab_type": "text"
      },
      "source": [
        "### Pre-Trained VGG19 from Torch Models\n",
        "\n",
        "Note that we only need the \"feature\" part, which includes all the CNNs and Pools. Just kick away the \"classifier\" part, which contains MLP and classifiers\n",
        "\n",
        "Also, need to normalize the images as in VGG19 training. Read the doc, we know it is using: mean = (0.485, 0.456, 0.406) and std dev = (0.229, 0.224, 0.225)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUtsD4v8nxUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_vgg19 = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "\n",
        "vgg19_norm_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "vgg19_norm_stdev = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "\n",
        "class vgg19_normalization(nn.Module):\n",
        "  def __init__(self, mean, std):\n",
        "    super(vgg19_normalization, self).__init__()\n",
        "    self.mean = torch.tensor(mean).view(-1,1,1)\n",
        "    self.std = torch.tensor(std).view(-1,1,1)\n",
        "  \n",
        "  def forward(self, img):\n",
        "    return (img - self.mean) / self.std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1HKAGB-pzjt",
        "colab_type": "text"
      },
      "source": [
        "### Build the Network\n",
        "\n",
        "Just add loss layers after the vgg19 features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB0GX4TQLvXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_style_model_and_losses(pretrained_cnn, normal_mean, normal_stdev,\n",
        "                               stylier_img, content_img,\n",
        "                               stylier_layer=['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5'],\n",
        "                               content_layer=['conv_4']):\n",
        "  cnn = copy.deepcopy(pretrained_cnn)\n",
        "  normalization = vgg19_normalization(normal_mean,normal_stdev).to(device)\n",
        "\n",
        "  stylier_loss, content_loss = [], []\n",
        "\n",
        "  model = nn.Sequential(normalization)\n",
        "\n",
        "  i = 0\n",
        "  for layer in cnn.children():\n",
        "    # check if the layer is conv/relu/pool/batchnorm\n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "      i += 1\n",
        "      name = 'conv_{}'.format(i)\n",
        "    elif isinstance(layer, nn.ReLU):\n",
        "      name = 'relu_{}'.format(i)\n",
        "      layer = nn.ReLU(inplace=False)\n",
        "    elif isinstance(layer, nn.MaxPool2d):\n",
        "      name = 'pool_{}'.format(i)\n",
        "    elif isinstance(layer, nn.BatchNorm2d):\n",
        "      name = 'bn_{}'.format(i)\n",
        "    else:\n",
        "      raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
        "    \n",
        "    model.add_module(name, layer)\n",
        "\n",
        "    if name in content_layer: # add content loss if is in content layers\n",
        "      target = model(content_img).detach()\n",
        "      c_loss = ContentLoss(target)\n",
        "      model.add_module(\"content_loss_{}\".format(i), c_loss)\n",
        "      content_loss.append(c_loss)\n",
        "    \n",
        "    if name in stylier_layer:\n",
        "      target = model(stylier_img).detach()\n",
        "      s_loss = StylierLoss(target)\n",
        "      model.add_module(\"stylier_loss_{}\".format(i), s_loss)\n",
        "      stylier_loss.append(s_loss)\n",
        "    \n",
        "  # Trim the layers after the last content/style losses\n",
        "  for i in range(len(model)-1, -1, -1):\n",
        "    if isinstance(model[i],ContentLoss) or isinstance(model[i], StylierLoss):\n",
        "      break\n",
        "  model = model[:(i+1)]\n",
        "\n",
        "  return model, stylier_loss, content_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBvBjpmwSzu5",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer\n",
        "\n",
        "Use LBFGS, a \"flavored\" version of gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLxKfnEjS5Em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_input_optimizer(input_img):\n",
        "  optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
        "  return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tgl00Yz4Uq46",
        "colab_type": "text"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkOwHwwoUuyq",
        "colab_type": "text"
      },
      "source": [
        "### Define the training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-b9Qf6cS83-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
        "                       content_img, style_img, input_img, num_steps=600,\n",
        "                       style_weight=1000000, content_weight=1):\n",
        "    \"\"\"Run the style transfer.\"\"\"\n",
        "    print('Building the style transfer model..')\n",
        "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
        "        normalization_mean, normalization_std, style_img, content_img)\n",
        "    # print(model)\n",
        "    optimizer = get_input_optimizer(input_img)\n",
        "\n",
        "    print('Optimizing..')\n",
        "    run = [0]\n",
        "    while run[0] <= num_steps:\n",
        "\n",
        "        def closure():\n",
        "            # correct the values of updated input image\n",
        "            input_img.data.clamp_(0, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            model(input_img)\n",
        "            style_score = 0\n",
        "            content_score = 0\n",
        "\n",
        "            for sl in style_losses:\n",
        "                style_score += sl.loss\n",
        "            for cl in content_losses:\n",
        "                content_score += cl.loss\n",
        "\n",
        "            style_score *= style_weight\n",
        "            content_score *= content_weight\n",
        "\n",
        "            loss = style_score + content_score\n",
        "            loss.backward()\n",
        "\n",
        "            run[0] += 1\n",
        "            if run[0] % 100 == 0:\n",
        "                print(\"run {}: Style Loss : {:4f} Content Loss: {:4f}\".format(run,style_score.item(), content_score.item()))\n",
        "\n",
        "            return style_score + content_score\n",
        "\n",
        "        optimizer.step(closure)\n",
        "\n",
        "    # a last correction...\n",
        "    input_img.data.clamp_(0, 1)\n",
        "\n",
        "    return input_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eHwNQAXdist",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = content_img.clone()\n",
        "output = run_style_transfer(cnn_vgg19,vgg19_norm_mean,vgg19_norm_stdev, content_img, stylier_img, input_img,num_steps=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4FRwkbIV4zJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_show(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmPxgMoSWDhO",
        "colab_type": "text"
      },
      "source": [
        "## Runing Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlgvFiAxWGO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"working on {}\".format(device))\n",
        "print(\"working with images of size {}\".format(imsize))\n",
        "\n",
        "content_names = []\n",
        "stylier_names = []\n",
        "painted_results = []\n",
        "\n",
        "cnn_vgg19 = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "vgg19_norm_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "vgg19_norm_stdev = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "\n",
        "for c_name, s_name in zip(content_names, stylier_names):\n",
        "  print(\"using {} as content iamge and {} as stylier\\n\".format(c_name, s_name))\n",
        "  c_img = image_loader(\"img_data/{}.jpg\".format(c_name))\n",
        "  s_img = image_loader(\"img_data/{}.jpg\".format(s_name))\n",
        "  assert c_img.size() == s_img.size(), \"content image and stylier image should be of same size\"\n",
        "  i_img = c_img.clone()\n",
        "  painted_img = run_style_transfer(cnn_vgg19,vgg19_norm_mean,vgg19_norm_stdev, content_img, stylier_img, input_img,num_steps=800)\n",
        "  image_show(painted_img, title=\"c-{}_s-{}\",save_fig=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}